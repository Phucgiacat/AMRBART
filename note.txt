## 1
Sửa file main.py Mở file /content/AMR_reverse_graph_linearization/fine-tune/main.py và tìm đến dòng 17.
từ:
from datasets import load_dataset, load_metric, load_from_disk

thành:
from datasets import load_dataset, load_from_disk
from evaluate import load as load_metric


## 2
/content/AMR_reverse_graph_linearization/fine-tune/common/constant.py
dòng 13

# --- CODE CŨ (ĐANG LỖI) ---
from transformers import (
    AutoConfig,
    AutoModelForSeq2SeqLM,
    AutoTokenizer,
    AdamW,  # <--- Nguyên nhân lỗi nằm ở đây
    get_linear_schedule_with_warmup,
    # ... các dòng khác
)



# --- CODE MỚI (ĐÃ SỬA) ---
from torch.optim import AdamW  # <--- Thêm dòng này (lấy AdamW từ PyTorch)

from transformers import (
    AutoConfig,
    AutoModelForSeq2SeqLM,
    AutoTokenizer,
    # AdamW,  <--- Xóa hoặc comment dòng này lại
    get_linear_schedule_with_warmup,
    # ...
)



# 3
Mở file /content/AMR_reverse_graph_linearization/fine-tune/common/training_args.py.
Tìm:
# --- TÌM ĐOẠN NÀY ---
        if isinstance(self.sharded_ddp, bool):
            self.sharded_ddp = "simple" if self.sharded_ddp else ""
        if isinstance(self.sharded_ddp, str):
            self.sharded_ddp = [ShardedDDPOption(s) for s in self.sharded_ddp.split()]
        if self.sharded_ddp == [ShardedDDPOption.OFFLOAD]:
            raise ValueError(
                "`--sharded_ddp offload` can't work on its own. It needs to be added to `--sharded_ddp zero_dp_2` or "
                '`--sharded_ddp zero_dp_3`. For example, `--sharded_ddp "zero_dp_2 offload"`.'
            )
        elif len(self.sharded_ddp) > 1 and ShardedDDPOption.SIMPLE in self.sharded_ddp:
            raise ValueError("`--sharded_ddp simple` is not compatible with any other option.")
        elif ShardedDDPOption.ZERO_DP_2 in self.sharded_ddp and ShardedDDPOption.ZERO_DP_3 in self.sharded_ddp:
            raise ValueError("`--sharded_ddp zero_dp_2` is not compatible with `--sharded_ddp zero_dp_3`.")


Sửa:

# --- SỬA THÀNH ---
        # if isinstance(self.sharded_ddp, bool):
        #    self.sharded_ddp = "simple" if self.sharded_ddp else ""
        # if isinstance(self.sharded_ddp, str):
        #    # self.sharded_ddp = [ShardedDDPOption(s) for s in self.sharded_ddp.split()]
        #    pass 
        # ... (comment hết các dòng kiểm tra lỗi bên dưới luôn)


$4
/content/AMR_reverse_graph_linearization/fine-tune/common/training_args.py và thêm dòng này vào dòng đầu tiên (hoặc cùng chỗ với các import khác như json, math...):
from functools import cached_property

Xóa cached_property trong khối import cũ. Tìm đến đoạn import từ transformers.utils (khoảng dòng 33), trông nó sẽ như thế này:
from transformers.utils import (
    ExplicitEnum,
    cached_property,  # <--- BẠN CẦN XÓA DÒNG NÀY
    get_full_repo_name,
    is_sagemaker_dp_enabled,
    # ...
)